---
title: "Statistics and ML"
subtitle: "MSSP Practicum Discussion"
author: "MSSP Practicum II"
date: "2023-01-24"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

## Instructions

**Fork** the [`carvalho/stats-ml-practicum`](https://github.com/carvalho/stats-ml-practicum) repository at GitHub, and
**create a new branch with your BU login** to store your changes to the document.
Start by changing the `author`in the YAML header of the document to state **your name**.

Below we run some analyses and ask questions about them. As you run the code and
interpret the results within your group, write your answers to the questions following the analyses, but:

> You should submit your work as a **pull request** to the original repository!


## Introduction

In this project we study **tree canopy cover** as it varies with the
**relative distance** to a tree line boundary in urban forests. The dataset in
`stats-ml-canopy.RData` has three variables: `location` for the urban forest
where the canopy cover was observed, `distance` for the relative distance &mdash;
zero is inside the forest and one is outside (city) &mdash; and `cover` for the
canopy cover.

```{r}
load("stats-ml-canopy.RData")
(canopy <- as_tibble(canopy))

idx <- order(canopy$distance) # for plots below
ggplot(canopy, aes(distance, cover)) + geom_point(color = "gray")
```

As can be seen, there is a clear pattern here: the canopy cover starts high,
closer to 100% when inside the forest, but as the tree line recedes into the
city, the canopy cover approaches zero.

We are interested in two main tasks:

- **Understanding** this relationship more explicitly;
- **Predicting** the canopy cover at the assumed tree line boundary when
`distance` is 0.5.

To this end, we explore four approaches below.

## Statistics 1: Linear Fit

```{r stats1}
m <- glm(cover ~ distance, data = canopy, family = quasibinomial)
ggplot(canopy, aes(distance, cover)) + geom_point(col = "gray") +
  geom_line(aes(distance[idx], fitted(m)[idx]))
predict(m, data.frame(distance = 0.5), se = TRUE, type = "response")
```

Questions and tasks:

- Comment on the fit, plot residuals and comment on them.
```{r}
plot(m, 1)
```

- Comment on the prediction; does it seem reasonable?
##The residuals are evenly distributed around two sides of 0, so the plot is reasonable, these data set has obvious linear relation. It's resonable since the residuals become smaller along with distance grow.





## ML 1: LOESS

```{r ml1}
m <- loess(cover ~ distance, data = canopy)
ggplot(canopy, aes(distance, cover)) + geom_point(col = "gray") +
  geom_line(aes(distance[idx], fitted(m)[idx]))
predict(m, data.frame(distance = 0.5), se = TRUE)
```

Questions and tasks:

- Check the definition of the `loess` function; how does it differ from the previous approach?
##LOESES fit a polynomial surface determined by one or more numerical predictors, using local fitting. The `loses` only fit local data.
- Comment on the fit; does it seem reasonable?
##It seems resonable since inside the forest the canopy cover approach to 100% and decede among the tree line near the city
- Comment on the prediction, including the SE.


## ML 2: Random Forest

```{r ml2,message=FALSE}
library(randomForest)
m <- randomForest(cover ~ distance, data = canopy)
ggplot(canopy, aes(distance, cover)) + geom_point(col = "gray") +
  geom_line(aes(distance[idx], predict(m)[idx]))
predict(m, data.frame(distance = 0.5), se = TRUE)
```

Questions and tasks:

- Check what `randomForest` does; what is **keyword** here?
##Random forests build prediction models by sampling objects and variables, that is, generating multiple decision trees and classifying objects in turn. Finally, the classification results of each decision tree were summarized, and the mode category in all predicted categories was the category of the object predicted by random forest, and the classification accuracy was improved.
- Comment on the fit; how does it differ from the previous fits?
##It fit well, however the trend become segmented. Because of the large sample size of the data, the number of nodes should be greater than four, so as to better take into account the smoothness of the fitted line and avoid over-fitting
- Comment on the prediction; how would you obtain a measure of uncertainty?
##It seems fit more precisly, since we verge the range to 0.5

## Statistics 2: Cubic Fit

```{r stats2}
m <- glm(cover ~ poly(distance, 3), data = canopy, family = quasibinomial)
ggplot(canopy, aes(distance, cover)) + geom_point(col = "gray") +
  geom_line(aes(distance[idx], fitted(m)[idx]))
predict(m, data.frame(distance = 0.5), se = TRUE, type = "response")
plot(m,1)
```

Questions and tasks:

- Comment on the fit and compare it to the first model; plot and check residuals.
##The cubic fit not  suitable for this samplet, because the sample size is 3000 which is bigger than 100, so the node number should be more than 4. And the residual plot is quite similar to the linear regression model.
- Comment on the prediction and compare it to previous results.
- How would you know that a cubic fit is good enough?
##when the p-value smaller than 0.05 it seems that the model fit well enough.

## Discussion

Let's try to connect all lessons learned from your work and the discussions.
Elaborate more on the following questions:

- How would you know that the predictions are *reliable*?
##Depends on the residual plots
- How would you test that the cover is exactly 50% at the boundary (`distance` = 0.5)? Which approaches would make the test easier to perform?
##I think the randomforset is easier to perform.
- How would you incorporate `location` in your analyses? How would you know that
it is meaningful to use it?
##
