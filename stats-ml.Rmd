---
title: "Statistics and ML"
subtitle: "MSSP Practicum Discussion"
author: "Tao Guo"
date: "2023-01-24"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)

```

## Instructions

**Fork** the [`carvalho/stats-ml-practicum`](https://github.com/carvalho/stats-ml-practicum) repository at GitHub, and
**create a new branch with your BU login** to store your changes to the document.
Start by changing the `author`in the YAML header of the document to state **your name**.

Below we run some analyses and ask questions about them. As you run the code and
interpret the results within your group, write your answers to the questions following the analyses, but:

> You should submit your work as a **pull request** to the original repository!


## Introduction

In this project we study **tree canopy cover** as it varies with the
**relative distance** to a tree line boundary in urban forests. The dataset in
`stats-ml-canopy.RData` has three variables: `location` for the urban forest
where the canopy cover was observed, `distance` for the relative distance &mdash;
zero is inside the forest and one is outside (city) &mdash; and `cover` for the
canopy cover.

```{r}
load("stats-ml-canopy.RData")
(canopy <- as_tibble(canopy))

idx <- order(canopy$distance) # for plots below
ggplot(canopy, aes(distance , cover)) + geom_point(color = "gray")
```

As can be seen, there is a clear pattern here: the canopy cover starts high,
closer to 100% when inside the forest, but as the tree line recedes into the
city, the canopy cover approaches zero.

We are interested in two main tasks:

- **Understanding** this relationship more explicitly;
- **Predicting** the canopy cover at the assumed tree line boundary when
`distance` is 0.5.

To this end, we explore four approaches below.

## Statistics 1: Linear Fit

```{r stats1}
m <- glm(cover ~ distance, data = canopy, family = quasibinomial)
ggplot(canopy, aes(distance, cover)) + geom_point(col = "gray") +
  geom_line(aes(distance[idx], fitted(m)[idx]))
predict(m, data.frame(distance = 0.5), se = TRUE, type = "response")
```


Questions and tasks:

- Comment on the fit, plot residuals and comment on them.

It do not fit well because it is not equal variance and have pattern. 
```{r}
plot(m)
```

- Comment on the prediction; does it seem reasonable?
I think this prediction is not well, it is residual is relatively large. 

## ML 1: LOESS

```{r ml1}
m <- loess(cover ~ distance, data = canopy)
ggplot(canopy, aes(distance, cover)) + geom_point(col = "gray") +
  geom_line(aes(distance[idx], fitted(m)[idx]))
predict(m, data.frame(distance = 0.5), se = TRUE)
```

Questions and tasks:

- Check the definition of the `loess` function; how does it differ from the previous approach?

loess is local regression method, which only fit at target point using only the nearby points. 

- Comment on the fit; does it seem reasonable?
In tail of curve, the fit is not well because the cover rate increase, which is not reasonable.

- Comment on the prediction, including the SE.
At the 0.5 distance, we have 50% to see cover, the se is very small. 

## ML 2: Random Forest

```{r ml2,message=FALSE}
library(randomForest)
m <- randomForest(cover ~ distance, data = canopy)
ggplot(canopy, aes(distance, cover)) + geom_point(col = "gray") +
  geom_line(aes(distance[idx], predict(m)[idx]))
predict(m, data.frame(distance = 0.5), se = TRUE)
```

Questions and tasks:

- Check what `randomForest` does; what is **keyword** here?
A random forest is a type of machine learning algorithm used for classification and regression tasks. The "random" part of the name refers to the way the algorithm creates multiple decision trees during training, and "forest" refers to the collection of these trees. The keyword in this context refers to the input variable(s) used by the algorithm to make predictions.
- Comment on the fit; how does it differ from the previous fits?
This fit is closer to first one, but different to second one. 
- Comment on the prediction; how would you obtain a measure of uncertainty?
we can use bootstrap 


## Statistics 2: Cubic Fit

```{r stats2}
m1 <- glm(cover ~ poly(distance, 3), data = canopy, family = quasibinomial)
ggplot(canopy, aes(distance, cover)) + geom_point(col = "gray") +
  geom_line(aes(distance[idx], fitted(m)[idx]))
predict(m1, data.frame(distance = 0.5), se = TRUE, type = "response")
```
```{r}
summary(m)
```


```{r}
summary(m1)

plot(m1,1)
```

Questions and tasks:

- Comment on the fit and compare it to the first model; plot and check residuals.
These two model is closer to each other, but ploy model is little better than first one because it residuals is smaller. 
- Comment on the prediction and compare it to previous results.
this prediction is more closer to 0.5 than previous results.
- How would you know that a cubic fit is good enough?
From plot of fit line and real data, the fit line follow the pattern of data

## Discussion

Let's try to connect all lessons learned from your work and the discussions.
Elaborate more on the following questions:

- How would you know that the predictions are *reliable*?
In data, at 0.5 distance,  cover also closer to 50% range, prediction is in this range. 
- How would you test that the cover is exactly 50% at the boundary (`distance` = 0.5)? Which approaches would make the test easier to perform?

- How would you incorporate `location` in your analyses? How would you know that
it is meaningful to use it?
I may add location to in my model, I may check the p value to check the meaningfulness. 