---
title: "Statistics and ML"
subtitle: "MSSP Practicum Discussion"
author: "Zhi Tu"
date: "2023-01-24"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)

```

## Instructions 

**Fork** the [`carvalho/stats-ml-practicum`](https://github.com/carvalho/stats-ml-practicum) repository at GitHub, and
**create a new branch with your BU login** to store your changes to the document.
Start by changing the `author`in the YAML header of the document to state **your name**.

Below we run some analyses and ask questions about them. As you run the code and
interpret the results within your group, write your answers to the questions following the analyses, but:

> You should submit your work as a **pull request** to the original repository!


## Introduction

In this project we study **tree canopy cover** as it varies with the
**relative distance** to a tree line boundary in urban forests. The dataset in
`stats-ml-canopy.RData` has three variables: `location` for the urban forest
where the canopy cover was observed, `distance` for the relative distance &mdash;
zero is inside the forest and one is outside (city) &mdash; and `cover` for the
canopy cover.

```{r}
load("stats-ml-canopy.RData")
(canopy <- as_tibble(canopy))

idx <- order(canopy$distance) # for plots below
ggplot(canopy, aes(distance, cover)) + geom_point(color = "gray")
```

As can be seen, there is a clear pattern here: the canopy cover starts high,
closer to 100% when inside the forest, but as the tree line recedes into the
city, the canopy cover approaches zero.

We are interested in two main tasks:

- **Understanding** this relationship more explicitly;
- **Predicting** the canopy cover at the assumed tree line boundary when
`distance` is 0.5.

To this end, we explore four approaches below.

## Statistics 1: Linear Fit

```{r stats1}
m <- glm(cover ~ distance, data = canopy, family = quasibinomial)
ggplot(canopy, aes(distance, cover)) + geom_point(col = "gray") +
  geom_line(aes(distance[idx], fitted(m)[idx]))
predict(m, data.frame(distance = 0.5), se = TRUE, type = "response")
```

Questions and tasks:

- Comment on the fit, plot residuals and comment on them.
The graph looks as a good fit to data.
- Comment on the prediction; does it seem reasonable?
The standard error is low and residual.scale is small so it seems reasonable.

## ML 1: LOESS

```{r ml1}
m <- loess(cover ~ distance, data = canopy)
ggplot(canopy, aes(distance, cover)) + geom_point(col = "gray") +
  geom_line(aes(distance[idx], fitted(m)[idx]))
predict(m, data.frame(distance = 0.5), se = TRUE)
```

Questions and tasks:

- Check the definition of the `loess` function; how does it differ from the previous approach?
Loess function is non-parametric. And it also has a weight parameter.
- Comment on the fit; does it seem reasonable?
Yes. The graph looks as a good fit to data.
- Comment on the prediction, including the SE.
The prediction seems better than the linear regression model since it has lower standard error and residual scale value.

## ML 2: Random Forest

```{r ml2,message=FALSE}
library(randomForest)
m <- randomForest(cover ~ distance, data = canopy)
ggplot(canopy, aes(distance, cover)) + geom_point(col = "gray") +
  geom_line(aes(distance[idx], predict(m)[idx]))
predict(m, data.frame(distance = 0.5), se = TRUE)
```

Questions and tasks:

- Check what `randomForest` does; what is **keyword** here?
Predict outcome using decision trees. The keyword is random.
- Comment on the fit; how does it differ from the previous fits?
The graph looks as a good fit to data. However, the line seems zigzag instead of a smooth line.
- Comment on the prediction; how would you obtain a measure of uncertainty?
The predicted value seems higher than the previous two methods.

## Statistics 2: Cubic Fit

```{r stats2}
m <- glm(cover ~ poly(distance, 3), data = canopy, family = quasibinomial)
ggplot(canopy, aes(distance, cover)) + geom_point(col = "gray") +
  geom_line(aes(distance[idx], fitted(m)[idx]))
predict(m, data.frame(distance = 0.5), se = TRUE, type = "response")
```

Questions and tasks:

- Comment on the fit and compare it to the first model; plot and check residuals.
The graph seems similar to the first model. However, when we check the residual plot, we can clearly see the vertical pattern as different locations of canopy.
```{r}
residuals <- residuals(m)
fitted_values <- fitted(m)
residual_plot_data <- data.frame(Fitted = fitted_values, Residuals = residuals)
ggplot(residual_plot_data, aes(Fitted, Residuals)) +
  geom_point(alpha=0.3) +
  xlab("Fitted Values") +
  ylab("Residuals")
```


- Comment on the prediction and compare it to previous results.
The result seems better than random forest but not the first two models
- How would you know that a cubic fit is good enough?
By checking the residual plot.

## Discussion

Let's try to connect all lessons learned from your work and the discussions.
Elaborate more on the following questions:

- How would you know that the predictions are *reliable*?
We can check the model fit by check difference between predicted values vs. actual value.
- How would you test that the cover is exactly 50% at the boundary (`distance` = 0.5)? Which approaches would make the test easier to perform?
We can setup the null hypothesis test. Specifically, the predicted value of the cover at distance = 0.5 is equal to 50%, and the alternative hypothesis is that the predicted value is not equal to 50%.

- How would you incorporate `location` in your analyses? How would you know that
it is meaningful to use it?
I would include the location as leveled categorical predictors. By checking the residual plot, we can see that we have eliminate the vertical pattern by incorporating the `location` variable.
```{r}
canopy$location <- as.factor(canopy$location)
m <- glm(cover ~ distance+location, data = canopy, family = quasibinomial)
residuals <- residuals(m)
fitted_values <- fitted(m)
residual_plot_data <- data.frame(Fitted = fitted_values, Residuals = residuals)
ggplot(residual_plot_data, aes(Fitted, Residuals)) +
  geom_point(alpha=0.3) +
  xlab("Fitted Values") +
  ylab("Residuals")
```




