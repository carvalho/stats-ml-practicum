---
title: "Statistics and ML"
author: "Jiaqi Sun"
date: "2023-01-24"
output:
  pdf_document: default
  html_document: default
subtitle: MSSP Practicum Discussion
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

## Instructions

**Fork** the [`carvalho/stats-ml-practicum`](https://github.com/carvalho/stats-ml-practicum) repository at GitHub, and
**create a new branch with your BU login** to store your changes to the document.
Start by changing the `author`in the YAML header of the document to state **your name**.

Below we run some analyses and ask questions about them. As you run the code and
interpret the results within your group, write your answers to the questions following the analyses, but:

> You should submit your work as a **pull request** to the original repository!


## Introduction

In this project we study **tree canopy cover** as it varies with the
**relative distance** to a tree line boundary in urban forests. The dataset in
`stats-ml-canopy.RData` has three variables: `location` for the urban forest
where the canopy cover was observed, `distance` for the relative distance &mdash;
zero is inside the forest and one is outside (city) &mdash; and `cover` for the
canopy cover.

```{r}
load("stats-ml-canopy.RData")
(canopy <- as_tibble(canopy))

idx <- order(canopy$distance) # for plots below
ggplot(canopy, aes(distance, cover)) + geom_point(color = "gray")
```

As can be seen, there is a clear pattern here: the canopy cover starts high,
closer to 100% when inside the forest, but as the tree line recedes into the
city, the canopy cover approaches zero.

We are interested in two main tasks:

- **Understanding** this relationship more explicitly;
- **Predicting** the canopy cover at the assumed tree line boundary when
`distance` is 0.5.

To this end, we explore four approaches below.

## Statistics 1: Linear Fit

```{r stats1}
m <- glm(cover ~ distance, data = canopy, family = quasibinomial)
ggplot(canopy, aes(distance, cover)) + geom_point(col = "gray") +
  geom_line(aes(distance[idx], fitted(m)[idx]))
predict(m, data.frame(distance = 0.5), se = TRUE, type = "response")
```

Questions and tasks:

- Comment on the fit, plot residuals and comment on them.
```{r}
plot(m,1)
```

there is a clear pattern over variability, which shouldn't be.
The average on the mean is around 0, which is good.

- Comment on the prediction; does it seem reasonable?
The prediction do seem reasonable as it basically follows the actual data pattern.


## ML 1: LOESS

```{r ml1}
m <- loess(cover ~ distance, data = canopy)
ggplot(canopy, aes(distance, cover)) + geom_point(col = "gray") +
  geom_line(aes(distance[idx], fitted(m)[idx]))
predict(m, data.frame(distance = 0.5), se = TRUE)
```

Questions and tasks:

- Check the definition of the `loess` function; how does it differ from the previous approach?
loess: Local regression, locally fit a linear function just to the data in this region, fitted by weighted least squares. Very popular way of fitting nonlinear functions.

- Comment on the fit; does it seem reasonable?
```{r}
m
```
the residual standard error is low, gives a much better extrapolation at the boundary and suitable for curve pattern, it seems reasonable.

- Comment on the prediction, including the SE.
based on local regression thus more accurate, the se of fit becomes smaller than linear fit.


## ML 2: Random Forest

```{r ml2,message=FALSE}
library(randomForest)
m <- randomForest(cover ~ distance, data = canopy)
ggplot(canopy, aes(distance, cover)) + geom_point(col = "gray") +
  geom_line(aes(distance[idx], predict(m)[idx]))
predict(m, data.frame(distance = 0.5), se = TRUE)
```

Questions and tasks:

- Check what `randomForest` does; what is **keyword** here?
Random Forests: 
Random Forests build lots of bushy trees, and then average them to reduce the variance.
we build a number of decision trees, each time a split in a tree is considered, a random selection of m predictors is chosen as split candidates from the full set of p predictors. This de-correlates the trees to reduce the variance when averaging the trees.

- Comment on the fit; how does it differ from the previous fits?

The fit is good. It greatly reduce the error as the variance is reduced. 
The MSR is very low and 91.91% of the response variable is explained by our predictor, which is great.
The MSR and % variance explained are based on OOB or _out-of-bag_ estimates, a very clever device in random forests to get honest error estimates.
```{r}
m
```


- Comment on the prediction; how would you obtain a measure of uncertainty?
The prediction do seem reasonable as it basically follows the actual data pattern.
we can use bootstrap.

## Statistics 2: Cubic Fit

```{r stats2}
m <- glm(cover ~ poly(distance, 3), data = canopy, family = quasibinomial)
ggplot(canopy, aes(distance, cover)) + geom_point(col = "gray") +
  geom_line(aes(distance[idx], fitted(m)[idx]))
predict(m, data.frame(distance = 0.5), se = TRUE, type = "response")
```

Questions and tasks:

- Comment on the fit and compare it to the first model; plot and check residuals.
```{r}
summary(m)
plot(m,1)
```
there is a clear pattern over variability, which shouldn't be.
The average on the mean is around 0, which is good.

- Comment on the prediction and compare it to previous results.
the standard error is bigger than the linear and loess method.

- How would you know that a cubic fit is good enough?
we see that from the summary statistics, at poly=3, which is cubic fit, it is significant.

## Discussion

Let's try to connect all lessons learned from your work and the discussions.
Elaborate more on the following questions:

- How would you know that the predictions are *reliable*?
we can see from the MSE, R Squared value, P value and F1 score.

- How would you test that the cover is exactly 50% at the boundary (`distance` = 0.5)? Which approaches would make the test easier to perform?
I will use the loess, because local regression gives a much better extrapolation at the boundary and suitable for curve pattern

- How would you incorporate `location` in your analyses? How would you know that it is meaningful to use it?
use location as a factor in the predicting model.
see if its p value is significant.
```{r}
m <- glm(cover ~ distance + factor(location), data = canopy, family = quasibinomial)
summary(m)
```



